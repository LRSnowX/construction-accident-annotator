import json
import re
from copy import deepcopy
from pathlib import Path
from typing import Dict, List, Set, Tuple

import ahocorasick  # type: ignore
import jieba  # type: ignore

# è½»é‡åœ¨çº¿é€»è¾‘å›å½’æ¨¡å‹ï¼ˆéå»ºç­‘ä¸š=1ï¼Œå»ºç­‘ä¸š=0ï¼‰
# ç‰¹å¾ä¸ºå…³é”®è¯å­˜åœ¨ä¸å¦ï¼ˆ0/1ï¼‰

DEFAULT_BIAS = 0.0
LEARNING_RATE = 0.2

# è‡ªå­¦ä¹ å…³é”®è¯å‚æ•°
ALPHA = 1.0  # å¹³æ»‘
MIN_COUNT = 6  # æœ€å°å‡ºç°æ¬¡æ•°ï¼ˆæ€»è®¡ï¼‰
ABS_LOG_ODDS_THRESH = 1.0  # |log((pos+Î±)/(neg+Î±))| é˜ˆå€¼
MAX_TOKEN_LENGTH = 20


FEATURE_GROUPS = {
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šæµ·äº‹/èˆ¹èˆ¶
    "ship": [
        "èˆ¹èˆ¶",
        "æ¸”èˆ¹",
        "è´§è½®",
        "å®¢è½®",
        "è½®èˆ¹",
        "èˆ¹å‘˜",
        "èˆ¹é•¿",
        "èˆ¹åª",
        "æµ·äº‹",
        "æµ·ä¸Š",
        "èˆªé“",
        "èˆªè¡Œ",
        "é”šåœ°",
        "æ¸¯å£",
        "æµ·åŸŸ",
        "ç¢°æ’",
        "æœºèˆ±",
    ],
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šé“è·¯/é“è·¯äº¤é€š
    "traffic": [
        "äº¤é€šäº‹æ•…",
        "è½¦è¾†",
        "è½¿è½¦",
        "å…¬äº¤",
        "å¡è½¦",
        "é«˜é€Ÿ",
        "è·¯å£",
        "é©¾é©¶å‘˜",
        "ä¹˜å®¢",
        "è¿½å°¾",
        "ä¾§ç¿»",
        "è½¦ç¥¸",
        "åˆ—è½¦",
        "é“è·¯",
        "ç«è½¦",
        "åœ°é“",
    ],
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šçŸ¿å±±
    "mining": [
        "ç…¤çŸ¿",
        "çŸ¿äº•",
        "é‡‡åŒº",
        "äº•ä¸‹",
        "å··é“",
        "æ˜è¿›",
        "é¡¶æ¿",
        "çŸ¿å±±",
    ],
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šåŒ–å·¥/å±åŒ–
    "chemical": [
        "åŒ–å·¥",
        "å±åŒ–",
        "å±é™©åŒ–å­¦å“",
        "æ³„æ¼",
        "æœ‰æ¯’",
        "ç½ä½“",
        "æ§½ç½è½¦",
        "çˆ†ç‡ƒ",
        "ä¸­æ¯’",
    ],
    # å»ºç­‘ä¸šæ­£ç‰¹å¾ï¼šå‡ºç°åˆ™æ›´å¯èƒ½æ˜¯å»ºç­‘ä¸šï¼ˆå¯¹â€œéå»ºç­‘ä¸šâ€æ¦‚ç‡èµ·è´Ÿå‘ä½œç”¨ï¼‰
    "construction": [
        "æ–½å·¥",
        "å·¥åœ°",
        "å¡”åŠ",
        "è„šæ‰‹æ¶",
        "æ··å‡åœŸ",
        "æµ‡ç­‘",
        "åŠè£…",
        "æ¨¡æ¿",
        "åŸºå‘",
        "èµ·é‡",
        "æ–½å·¥ç°åœº",
        "ç­ç»„",
        "é’¢ç­‹",
        "æ¡©åŸº",
        "æ¡¥æ¢æ–½å·¥",
        "éš§é“æ–½å·¥",
        "é˜²æŠ¤æ£š",
    ],
}

# åˆå§‹åŒ–æƒé‡ï¼ˆå¯¹â€œéå»ºç­‘ä¸š=1â€çš„å€¾å‘ï¼‰
# éå»ºç­‘ä¸šç»„ï¼šæ­£æƒé‡ï¼›å»ºç­‘ä¸šç»„ï¼šè´Ÿæƒé‡
DEFAULT_WEIGHTS = {
    f: 1.5
    for f in (
        FEATURE_GROUPS["ship"]
        + FEATURE_GROUPS["traffic"]
        + FEATURE_GROUPS["mining"]
        + FEATURE_GROUPS["chemical"]
    )
}
DEFAULT_WEIGHTS.update({f: -2.0 for f in FEATURE_GROUPS["construction"]})


def _project_root() -> Path:
    # æœ¬æ–‡ä»¶ä½äºé¡¹ç›®æ ¹ç›®å½•
    return Path(__file__).resolve().parent


def _seed_config_path() -> Path:
    return _project_root() / "data" / "config" / "keyword_seeds.json"


BUILTIN_GROUPS = deepcopy(FEATURE_GROUPS)
BUILTIN_DEFAULT_WEIGHTS = deepcopy(DEFAULT_WEIGHTS)


# ä¾›ä¸»ç¨‹åºæ‰“å°çš„åˆå¹¶æ‘˜è¦
SEED_LOAD_SUMMARY = {
    "mode": "merge",
    "seeds_groups": 0,
    "seeds_weights": 0,
    "final_features": 0,
    "tokenizer": "jieba",
    "tokenizer_effective": "jieba",
}


# ---------------- åˆ†è¯èµ„æºä¸åˆ†è¯å™¨ ----------------
_STOPWORDS: Set[str] = {
    "äº‹æ•…",
    "å‘ç”Ÿ",
    "ç»è¿‡",
    "æƒ…å†µ",
    "æœ‰å…³",
    "äººå‘˜",
    "è´£ä»»",
    "å…¬å¸",
    "å•ä½",
    "ä½œä¸š",
    "å®‰å…¨",
    "ç®¡ç†",
    "æŠ¥å‘Š",
    "é€ æˆ",
    "å—ä¼¤",
    "æ­»äº¡",
    "è°ƒæŸ¥",
    "å»ºè®®",
    "ç›®å‰",
    "å½“åœº",
    "ç°åœº",
    "å¤„ç†",
    "éƒ¨é—¨",
    "å¹´",
    "æœˆ",
    "æ—¥",
    "æ—¶",
    "åˆ†",
    "æŸ",
    "è¯¥",
    "æœ¬",
    "ç­‰",
    "ä»¥åŠ",
    "å¹¶",
    "å¯¹",
    "ä¸­",
}


def _tokenizer_resources_dir() -> Path:
    return _project_root() / "data" / "config"


def _init_tokenizer_resources():
    # æ‰©å±•åœç”¨è¯
    sw = _tokenizer_resources_dir() / "stopwords.txt"
    if sw.exists():
        try:
            with open(sw, "r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if s:
                        _STOPWORDS.add(s)
        except Exception:
            pass
    # åŠ è½½ç”¨æˆ·è¯å…¸
    ud = _tokenizer_resources_dir() / "user_dict.txt"
    if ud.exists():
        try:
            jieba.load_userdict(str(ud))
        except Exception:
            pass


# ---------------- AC è‡ªåŠ¨æœºï¼ˆå…³é”®è¯åŒ¹é…ï¼‰ ----------------
_AC = None
_AC_KEYMAP: Dict[str, str] = {}


def _all_feature_keys() -> Set[str]:
    # ä½¿ç”¨ DEFAULT_WEIGHTS çš„é”®è¦†ç›–æ‰€æœ‰å†…ç½®/å¤–ç½®/å­¦ä¹ ç‰¹å¾
    return set(DEFAULT_WEIGHTS.keys()).union(
        FEATURE_GROUPS.get("_learned", []), FEATURE_GROUPS.get("_custom", [])
    )


def _rebuild_automaton():
    global _AC, _AC_KEYMAP
    A = ahocorasick.Automaton()
    keymap: Dict[str, str] = {}
    for k in _all_feature_keys():
        kl = str(k).lower()
        if not kl:
            continue
        if kl not in keymap:
            keymap[kl] = k
        A.add_word(kl, kl)  # å­˜ lower å€¼ä½œä¸º payload
    A.make_automaton()
    _AC = A
    _AC_KEYMAP = keymap


def _merge_seeds_into_defaults():
    path = _seed_config_path()
    if not path.exists():
        # æ— å¤–ç½®é…ç½®ï¼Œç»Ÿè®¡åŸºäºå†…ç½®
        SEED_LOAD_SUMMARY.update(
            {
                "mode": "builtin-only",
                "seeds_groups": 0,
                "seeds_weights": 0,
                "final_features": len(DEFAULT_WEIGHTS),
            }
        )
        return
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        SEED_LOAD_SUMMARY.update(
            {
                "mode": "load-error",
                "seeds_groups": 0,
                "seeds_weights": 0,
                "final_features": len(DEFAULT_WEIGHTS),
                "tokenizer": "jieba",
                "tokenizer_effective": "jieba",
            }
        )
        return

    groups = data.get("groups", {}) or {}
    weights_override = data.get("weights", {}) or {}
    mode = str(data.get("mode", "merge")).lower()

    # mode: replace -> å¤–ç½®ä½œä¸ºå”¯ä¸€çœŸç›¸ï¼Œé‡ç½®åˆ†ç»„ä¸é»˜è®¤æƒé‡
    if mode == "replace":
        FEATURE_GROUPS.clear()
        DEFAULT_WEIGHTS.clear()
        # ä¹Ÿå…è®¸å›é€€åˆ°æç®€ï¼šè‹¥å¤–ç½®ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å†…ç½®
        if not groups and not weights_override:
            FEATURE_GROUPS.update(deepcopy(BUILTIN_GROUPS))
            DEFAULT_WEIGHTS.update(deepcopy(BUILTIN_DEFAULT_WEIGHTS))
            mode = "replace-empty-fallback"

    # åˆå¹¶åˆ†ç»„å…³é”®è¯ï¼ˆå»é‡ï¼‰
    for group_name, key_list in groups.items():
        if not isinstance(key_list, list):
            continue
        existing = set(FEATURE_GROUPS.get(group_name, []))
        for k in key_list:
            try:
                k_l = str(k).strip()
            except Exception:
                continue
            if not k_l:
                continue
            existing.add(k_l)
        FEATURE_GROUPS[group_name] = sorted(existing)

    # å°†åˆ†ç»„æ–°å¢çš„å…³é”®è¯èµ‹é»˜è®¤æƒé‡
    for group_name, key_list in FEATURE_GROUPS.items():
        for k in key_list:
            if k not in DEFAULT_WEIGHTS:
                if group_name == "construction":
                    DEFAULT_WEIGHTS[k] = -2.0
                else:
                    DEFAULT_WEIGHTS[k] = 1.5

    # è‡ªå®šä¹‰æƒé‡ï¼šä¹Ÿå°†è¿™äº›è¯åŠ å…¥åˆ°ç‰¹å¾ç»„ï¼Œé¿å…åªåœ¨æƒé‡é‡Œä½†æ— æ³•è¢«æå–
    if weights_override:
        custom = set(FEATURE_GROUPS.get("_custom", []))
        for k, w in weights_override.items():
            try:
                k_l = str(k).strip()
                w_f = float(w)
            except Exception:
                continue
            if not k_l:
                continue
            DEFAULT_WEIGHTS[k_l] = w_f
            custom.add(k_l)
        FEATURE_GROUPS["_custom"] = sorted(custom)

    SEED_LOAD_SUMMARY.update(
        {
            "mode": mode,
            "seeds_groups": sum(len(v) for v in groups.values()) if groups else 0,
            "seeds_weights": len(weights_override),
            "final_features": len(DEFAULT_WEIGHTS),
            "tokenizer": "jieba",
            "tokenizer_effective": "jieba",
        }
    )
    # åˆå§‹åŒ–åˆ†è¯èµ„æºï¼ˆå›ºå®šä½¿ç”¨ jiebaï¼‰
    _init_tokenizer_resources()
    # æ„å»ºå…³é”®è¯ AC è‡ªåŠ¨æœº
    _rebuild_automaton()


# åœ¨æ¨¡å—å¯¼å…¥æ—¶åˆå¹¶å¤–ç½®ç§å­
_merge_seeds_into_defaults()


def get_seed_load_summary() -> str:
    m = SEED_LOAD_SUMMARY.get("mode", "")
    g = SEED_LOAD_SUMMARY.get("seeds_groups", 0)
    w = SEED_LOAD_SUMMARY.get("seeds_weights", 0)
    f = SEED_LOAD_SUMMARY.get("final_features", 0)
    t = SEED_LOAD_SUMMARY.get("tokenizer", "jieba")
    te = SEED_LOAD_SUMMARY.get("tokenizer_effective", "jieba")
    return f"å…³é”®è¯åŠ è½½: æ¨¡å¼={m}ï¼Œå¤–ç½®ç»„è¯={g}ï¼Œå¤–ç½®æƒé‡={w}ï¼Œæœ€ç»ˆç‰¹å¾æ•°={f}ï¼Œåˆ†è¯å™¨={t}({te})"


def sigmoid(x: float) -> float:
    if x < -50:
        return 0.0
    if x > 50:
        return 1.0
    from math import exp

    return 1.0 / (1.0 + exp(-x))


def model_path_from_base(base_output_path: str) -> Path:
    p = Path(base_output_path)
    return p.parent / f"{p.stem}_hint_model.json"


def load_hint_model(base_output_path: str) -> Dict:
    path = model_path_from_base(base_output_path)
    if not path.exists():
        return {
            "bias": DEFAULT_BIAS,
            "weights": DEFAULT_WEIGHTS.copy(),
            "token_stats": {},  # token -> {"pos": int, "neg": int}
        }
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # ä¿æŠ¤æ€§åˆå¹¶ï¼ˆæ–°å…³é”®è¯åŠ å…¥æ—¶ï¼‰
        weights = DEFAULT_WEIGHTS.copy()
        weights.update(data.get("weights", {}))
        return {
            "bias": data.get("bias", DEFAULT_BIAS),
            "weights": weights,
            "token_stats": data.get("token_stats", {}),
        }
    except Exception:
        return {
            "bias": DEFAULT_BIAS,
            "weights": DEFAULT_WEIGHTS.copy(),
            "token_stats": {},
        }


def save_hint_model(base_output_path: str, model: Dict):
    path = model_path_from_base(base_output_path)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(model, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def normalize_text(row) -> str:
    parts: List[str] = []
    for field in ("title", "category", "publish_date", "date"):
        if field in row.index and row[field] is not None:
            try:
                parts.append(str(row[field]))
            except Exception:
                pass
    # é™åˆ¶æ­£æ–‡å‰ 4000 å­—ï¼Œé¿å…ç‰¹åˆ«é•¿æ–‡æœ¬å½±å“æ€§èƒ½
    try:
        parts.append(str(row["full_text"])[:4000])
    except Exception:
        pass
    return "\n".join(parts).lower()


def extract_features(row) -> Dict[str, int]:
    text = normalize_text(row)
    t = text.lower()
    feats: Dict[str, int] = {}
    A = _AC
    if A is None:
        _rebuild_automaton()
        A = _AC
    if A is None:
        return feats
    matched: Set[str] = set()
    for _, payload in A.iter(t):
        # payload æ˜¯ lower å½¢å¼
        k = _AC_KEYMAP.get(payload, payload)
        matched.add(k)
    for k in matched:
        feats[k] = 1
    return feats


# ---------------- è‡ªå­¦ä¹ å…³é”®è¯ï¼ˆè½»é‡ï¼‰ ----------------


def _tokenize_for_learning(text: str) -> List[str]:
    # å›ºå®šä½¿ç”¨ jieba åˆ†è¯ï¼Œè¾…ä»¥è‹±æ–‡ã€æ•°å­—+ä¸­æ–‡æ¨¡å¼è¡¥å……
    tokens: List[str] = []
    # jieba ä¸­æ–‡/æ··åˆåˆ†è¯
    for w in jieba.lcut(text, cut_all=False):
        w = w.strip().lower()
        if not w:
            continue
        if w.isdigit():
            continue
        if w in _STOPWORDS:
            continue
        if len(w) < 2 or len(w) > MAX_TOKEN_LENGTH:
            continue
        tokens.append(w)
    # è‹±æ–‡/æ‹¼éŸ³å•è¯è¡¥å……ï¼ˆâ‰¥3ï¼‰
    t = text.lower()
    for m in re.finditer(r"[a-z]{3,}", t):
        tok = m.group(0)
        if tok in _STOPWORDS:
            continue
        if len(tok) <= MAX_TOKEN_LENGTH:
            tokens.append(tok)
    # æ•°å­—+ä¸­æ–‡/å­—æ¯ç»„åˆè¡¥å……ï¼ˆå¦‚ 588è½®ã€g123æ¬¡ï¼‰
    for m in re.finditer(r"\b\d{2,}[a-z\u4e00-\u9fff]+", t):
        tok = m.group(0)
        if len(tok) <= MAX_TOKEN_LENGTH:
            tokens.append(tok)
    return tokens


def update_token_stats(
    model: Dict, row, label_non_construction: int
) -> Dict[str, Tuple[int, int]]:
    """æ ¹æ®å½“å‰æ¡ˆä¾‹ä¸æ ‡ç­¾æ›´æ–° token ç»Ÿè®¡ï¼Œè¿”å›æœ¬æ¬¡å¢é‡ç”¨äºæ’¤é”€ã€‚
    label_non_construction: 1=éå»ºç­‘ä¸š, 0=å»ºç­‘ä¸š
    è¿”å›: {token: (d_pos, d_neg)}
    """
    if "token_stats" not in model:
        model["token_stats"] = {}
    text = normalize_text(row)
    toks = _tokenize_for_learning(text)
    delta: Dict[str, Tuple[int, int]] = {}
    for tok in toks:
        stat = model["token_stats"].setdefault(tok, {"pos": 0, "neg": 0})
        if label_non_construction == 1:
            stat["pos"] += 1
            delta[tok] = (1, 0)
        else:
            stat["neg"] += 1
            delta[tok] = (0, 1)
    return delta


def rollback_token_stats(model: Dict, delta: Dict[str, Tuple[int, int]]):
    if not delta:
        return
    stats = model.get("token_stats", {})
    for tok, (dp, dn) in delta.items():
        stat = stats.get(tok)
        if not stat:
            continue
        stat["pos"] = max(0, stat.get("pos", 0) - dp)
        stat["neg"] = max(0, stat.get("neg", 0) - dn)


def _log_odds(pos: int, neg: int, alpha: float = ALPHA) -> float:
    from math import log

    return log((pos + alpha) / (neg + alpha))


def maybe_expand_features(model: Dict, max_add: int = 3) -> List[str]:
    """åŸºäº token ç»Ÿè®¡ï¼Œç­›é€‰é«˜åˆ¤åˆ«åŠ›çš„ token åŠ¨æ€åŠ å…¥ä¸ºç‰¹å¾ï¼Œè¿”å›æ–°å¢åˆ—è¡¨ã€‚
    è§„åˆ™ï¼šæ€»é¢‘æ¬¡â‰¥MIN_COUNT ä¸” |log_odds|â‰¥é˜ˆå€¼ï¼›åˆå§‹æƒé‡=clip(log_odds, -3, 3)
    """
    stats = model.get("token_stats", {})
    weights = model.get("weights", {})
    candidates: List[Tuple[str, float, int]] = []  # (token, log_odds, total)
    for tok, st in stats.items():
        total = st.get("pos", 0) + st.get("neg", 0)
        if total < MIN_COUNT:
            continue
        lo = _log_odds(st.get("pos", 0), st.get("neg", 0))
        if abs(lo) >= ABS_LOG_ODDS_THRESH and tok not in weights:
            candidates.append((tok, lo, total))
    # å…ˆæŒ‰æ€»é¢‘æ¬¡ï¼Œå†æŒ‰ç»å¯¹ log_odds æ’åºï¼Œå–å‰è‹¥å¹²
    candidates.sort(key=lambda x: (x[2], abs(x[1])), reverse=True)
    added: List[str] = []
    # ç¡®ä¿å­˜åœ¨å­¦ä¹ ç»„
    learned_set = set(FEATURE_GROUPS.get("_learned", []))

    for tok, lo, _ in candidates[:max_add]:
        # åˆå§‹æƒé‡æŒ‰ log_odds æ˜ å°„ï¼Œå¹¶ç»“åˆæ–¹å‘ï¼šæ­£å€¼=éå»ºç­‘ä¸šï¼›è´Ÿå€¼=å»ºç­‘ä¸š
        init_w = max(-3.0, min(3.0, lo))
        weights[tok] = init_w
        learned_set.add(tok)
        added.append(tok)
    model["weights"] = weights
    FEATURE_GROUPS["_learned"] = sorted(learned_set)
    # æ–°å¢å­¦ä¹ ç‰¹å¾åé‡å»º AC è‡ªåŠ¨æœº
    if added:
        _rebuild_automaton()
    return added


def remove_learned_features(model: Dict, tokens: List[str]):
    """æ’¤é”€æ—¶ç§»é™¤æœ¬æ¬¡æ–°åŠ å…¥çš„ç‰¹å¾ï¼ˆä»…é™æœ¬æ¬¡æ–°å¢ï¼‰ã€‚"""
    if not tokens:
        return
    weights = model.get("weights", {})
    for t in tokens:
        if t in weights:
            # ä»…å½“è¯¥è¯æ¥è‡ªå­¦ä¹ ç»„ä¸”ä¸æ˜¯å†…ç½®/å¤–ç½®ç§å­æ—¶ç§»é™¤
            # ç®€å•ç­–ç•¥ï¼šå¦‚æœä¸åœ¨ä»»ä½•åˆ†ç»„ï¼ˆé™¤äº†_learnedå’Œ_customï¼‰åˆ™ç•™å­˜ï¼›å¦åˆ™ä¹Ÿå¯ä»¥ä¿ç•™ã€‚
            # è¿™é‡ŒæŒ‰ä¿å®ˆé€»è¾‘ï¼šåªä»weightsåˆ é™¤ï¼Œä¸åŠ¨åˆ†ç»„ä»¥é¿å…å½±å“å…¶ä»–æµç¨‹ã€‚
            try:
                del weights[t]
            except KeyError:
                pass
    model["weights"] = weights
    # ä»_learnedåˆ†ç»„ç§»é™¤
    learned = set(FEATURE_GROUPS.get("_learned", []))
    for t in tokens:
        learned.discard(t)
    FEATURE_GROUPS["_learned"] = sorted(learned)
    # å­¦ä¹ ç‰¹å¾ç§»é™¤åé‡å»º AC è‡ªåŠ¨æœº
    if tokens:
        _rebuild_automaton()


def predict_non_construction_proba(
    model: Dict, features: Dict[str, int]
) -> Tuple[float, List[Tuple[str, float]]]:
    w = model["weights"]
    z = model.get("bias", 0.0)
    contributions: List[Tuple[str, float]] = []
    for name, x in features.items():
        if not x:
            continue
        c = w.get(name, 0.0) * x
        contributions.append((name, c))
        z += c
    p = sigmoid(z)
    # æ ¹æ®ç»å¯¹è´¡çŒ®æ’åºï¼Œå–å‰å‡ é¡¹è§£é‡Š
    contributions.sort(key=lambda t: abs(t[1]), reverse=True)
    return p, contributions[:5]


def update_model_online(
    model: Dict, features: Dict[str, int], label_non_construction: int
) -> Dict[str, float]:
    # è¿”å›æœ¬æ¬¡åº”ç”¨åˆ°æƒé‡çš„å¢é‡ï¼ˆç”¨äºæ’¤é”€ï¼‰
    p, _ = predict_non_construction_proba(model, features)
    error = label_non_construction - p  # y - p
    delta_bias = LEARNING_RATE * error
    model["bias"] = model.get("bias", 0.0) + delta_bias
    delta_w: Dict[str, float] = {}
    for name, x in features.items():
        if not x:
            continue
        dw = LEARNING_RATE * error * x
        model["weights"][name] = model["weights"].get(name, 0.0) + dw
        delta_w[name] = dw
    # è¿”å›å¢é‡ä»¥ä¾¿æ’¤é”€
    return {"bias": delta_bias, "weights": delta_w}


def rollback_update(model: Dict, delta: Dict[str, float]):
    model["bias"] = model.get("bias", 0.0) - delta.get("bias", 0.0)
    for name, dw in delta.get("weights", {}).items():
        model["weights"][name] = model["weights"].get(name, 0.0) - dw


def format_hint_line(prob: float, contributors: List[Tuple[str, float]]) -> str:
    pct = int(round(prob * 100))
    reason_keys = [f for f, _ in contributors if _ != 0]
    if reason_keys:
        reason = "ï¼Œä¾æ®: " + ", ".join(reason_keys[:3])
    else:
        reason = ""
    return f"ğŸ” æ™ºèƒ½æç¤º: éå»ºç­‘ä¸šæ¦‚ç‡çº¦ {pct}%{reason}"
