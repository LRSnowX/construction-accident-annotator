import json
import re
from collections import defaultdict
from copy import deepcopy
from math import exp, log, sqrt
from pathlib import Path
from typing import Dict, List, Set, Tuple

import ahocorasick  # type: ignore
import jieba  # type: ignore

# è½»é‡åœ¨çº¿é€»è¾‘å›å½’æ¨¡å‹ï¼ˆéå»ºç­‘ä¸š=1ï¼Œå»ºç­‘ä¸š=0ï¼‰
# ç‰¹å¾ä¸ºå…³é”®è¯å­˜åœ¨ä¸å¦ï¼ˆ0/1ï¼‰

DEFAULT_BIAS = 0.0
LEARNING_RATE = 0.2

# è‡ªå­¦ä¹ å…³é”®è¯å‚æ•°
ALPHA = 1.0  # å¹³æ»‘
MIN_COUNT = 6  # æœ€å°å‡ºç°æ¬¡æ•°ï¼ˆæ€»è®¡ï¼‰
ABS_LOG_ODDS_THRESH = 1.0  # |log((pos+Î±)/(neg+Î±))| é˜ˆå€¼
MAX_TOKEN_LENGTH = 20


FEATURE_GROUPS = {
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šæµ·äº‹/èˆ¹èˆ¶
    "ship": [
        "èˆ¹èˆ¶",
        "æ¸”èˆ¹",
        "è´§è½®",
        "å®¢è½®",
        "è½®èˆ¹",
        "èˆ¹å‘˜",
        "èˆ¹é•¿",
        "èˆ¹åª",
        "æµ·äº‹",
        "æµ·ä¸Š",
        "èˆªé“",
        "èˆªè¡Œ",
        "é”šåœ°",
        "æ¸¯å£",
        "æµ·åŸŸ",
        "ç¢°æ’",
        "æœºèˆ±",
    ],
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šé“è·¯/é“è·¯äº¤é€š
    "traffic": [
        "äº¤é€šäº‹æ•…",
        "è½¦è¾†",
        "è½¿è½¦",
        "å…¬äº¤",
        "å¡è½¦",
        "é«˜é€Ÿ",
        "è·¯å£",
        "é©¾é©¶å‘˜",
        "ä¹˜å®¢",
        "è¿½å°¾",
        "ä¾§ç¿»",
        "è½¦ç¥¸",
        "åˆ—è½¦",
        "é“è·¯",
        "ç«è½¦",
        "åœ°é“",
    ],
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šçŸ¿å±±
    "mining": [
        "ç…¤çŸ¿",
        "çŸ¿äº•",
        "é‡‡åŒº",
        "äº•ä¸‹",
        "å··é“",
        "æ˜è¿›",
        "é¡¶æ¿",
        "çŸ¿å±±",
    ],
    # éå»ºç­‘ä¸šå¼ºç‰¹å¾ï¼šåŒ–å·¥/å±åŒ–
    "chemical": [
        "åŒ–å·¥",
        "å±åŒ–",
        "å±é™©åŒ–å­¦å“",
        "æ³„æ¼",
        "æœ‰æ¯’",
        "ç½ä½“",
        "æ§½ç½è½¦",
        "çˆ†ç‡ƒ",
        "ä¸­æ¯’",
    ],
    # å»ºç­‘ä¸šæ­£ç‰¹å¾ï¼šå‡ºç°åˆ™æ›´å¯èƒ½æ˜¯å»ºç­‘ä¸šï¼ˆå¯¹â€œéå»ºç­‘ä¸šâ€æ¦‚ç‡èµ·è´Ÿå‘ä½œç”¨ï¼‰
    "construction": [
        "æ–½å·¥",
        "å·¥åœ°",
        "å¡”åŠ",
        "è„šæ‰‹æ¶",
        "æ··å‡åœŸ",
        "æµ‡ç­‘",
        "åŠè£…",
        "æ¨¡æ¿",
        "åŸºå‘",
        "èµ·é‡",
        "æ–½å·¥ç°åœº",
        "ç­ç»„",
        "é’¢ç­‹",
        "æ¡©åŸº",
        "æ¡¥æ¢æ–½å·¥",
        "éš§é“æ–½å·¥",
        "é˜²æŠ¤æ£š",
    ],
}

# åˆå§‹åŒ–æƒé‡ï¼ˆå¯¹â€œéå»ºç­‘ä¸š=1â€çš„å€¾å‘ï¼‰
# éå»ºç­‘ä¸šç»„ï¼šæ­£æƒé‡ï¼›å»ºç­‘ä¸šç»„ï¼šè´Ÿæƒé‡
DEFAULT_WEIGHTS = {
    f: 1.5
    for f in (
        FEATURE_GROUPS["ship"]
        + FEATURE_GROUPS["traffic"]
        + FEATURE_GROUPS["mining"]
        + FEATURE_GROUPS["chemical"]
    )
}
DEFAULT_WEIGHTS.update({f: -2.0 for f in FEATURE_GROUPS["construction"]})


def _project_root() -> Path:
    # æœ¬æ–‡ä»¶ä½äºé¡¹ç›®æ ¹ç›®å½•
    return Path(__file__).resolve().parent


def _seed_config_path() -> Path:
    return _project_root() / "data" / "config" / "keyword_seeds.json"


BUILTIN_GROUPS = deepcopy(FEATURE_GROUPS)
BUILTIN_DEFAULT_WEIGHTS = deepcopy(DEFAULT_WEIGHTS)


# ä¾›ä¸»ç¨‹åºæ‰“å°çš„åˆå¹¶æ‘˜è¦
SEED_LOAD_SUMMARY = {
    "mode": "merge",
    "seeds_groups": 0,
    "seeds_weights": 0,
    "final_features": 0,
    "tokenizer": "jieba",
    "tokenizer_effective": "jieba",
}


# ---------------- åˆ†è¯èµ„æºä¸åˆ†è¯å™¨ ----------------
_STOPWORDS: Set[str] = {
    "äº‹æ•…",
    "å‘ç”Ÿ",
    "ç»è¿‡",
    "æƒ…å†µ",
    "æœ‰å…³",
    "äººå‘˜",
    "è´£ä»»",
    "å…¬å¸",
    "å•ä½",
    "ä½œä¸š",
    "å®‰å…¨",
    "ç®¡ç†",
    "æŠ¥å‘Š",
    "é€ æˆ",
    "å—ä¼¤",
    "æ­»äº¡",
    "è°ƒæŸ¥",
    "å»ºè®®",
    "ç›®å‰",
    "å½“åœº",
    "ç°åœº",
    "å¤„ç†",
    "éƒ¨é—¨",
    "å¹´",
    "æœˆ",
    "æ—¥",
    "æ—¶",
    "åˆ†",
    "æŸ",
    "è¯¥",
    "æœ¬",
    "ç­‰",
    "ä»¥åŠ",
    "å¹¶",
    "å¯¹",
    "ä¸­",
}


def _tokenizer_resources_dir() -> Path:
    return _project_root() / "data" / "config"


def _init_tokenizer_resources():
    # æ‰©å±•åœç”¨è¯
    sw = _tokenizer_resources_dir() / "stopwords.txt"
    if sw.exists():
        try:
            with open(sw, "r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if s:
                        _STOPWORDS.add(s)
        except Exception:
            pass
    # åŠ è½½ç”¨æˆ·è¯å…¸
    ud = _tokenizer_resources_dir() / "user_dict.txt"
    if ud.exists():
        try:
            jieba.load_userdict(str(ud))
        except Exception:
            pass


# ---------------- AC è‡ªåŠ¨æœºï¼ˆå…³é”®è¯åŒ¹é…ï¼‰ ----------------
_AC = None
_AC_KEYMAP: Dict[str, str] = {}


def _all_feature_keys() -> Set[str]:
    # ä½¿ç”¨ DEFAULT_WEIGHTS çš„é”®è¦†ç›–æ‰€æœ‰å†…ç½®/å¤–ç½®/å­¦ä¹ ç‰¹å¾
    return set(DEFAULT_WEIGHTS.keys()).union(
        FEATURE_GROUPS.get("_learned", []), FEATURE_GROUPS.get("_custom", [])
    )


def _rebuild_automaton():
    global _AC, _AC_KEYMAP
    A = ahocorasick.Automaton()
    keymap: Dict[str, str] = {}
    for k in _all_feature_keys():
        kl = str(k).lower()
        if not kl:
            continue
        if kl not in keymap:
            keymap[kl] = k
        A.add_word(kl, kl)  # å­˜ lower å€¼ä½œä¸º payload
    A.make_automaton()
    _AC = A
    _AC_KEYMAP = keymap


def _merge_seeds_into_defaults():
    path = _seed_config_path()
    if not path.exists():
        # æ— å¤–ç½®é…ç½®ï¼Œç»Ÿè®¡åŸºäºå†…ç½®
        SEED_LOAD_SUMMARY.update(
            {
                "mode": "builtin-only",
                "seeds_groups": 0,
                "seeds_weights": 0,
                "final_features": len(DEFAULT_WEIGHTS),
            }
        )
        return
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        SEED_LOAD_SUMMARY.update(
            {
                "mode": "load-error",
                "seeds_groups": 0,
                "seeds_weights": 0,
                "final_features": len(DEFAULT_WEIGHTS),
                "tokenizer": "jieba",
                "tokenizer_effective": "jieba",
            }
        )
        return

    groups = data.get("groups", {}) or {}
    weights_override = data.get("weights", {}) or {}
    mode = str(data.get("mode", "merge")).lower()

    # mode: replace -> å¤–ç½®ä½œä¸ºå”¯ä¸€çœŸç›¸ï¼Œé‡ç½®åˆ†ç»„ä¸é»˜è®¤æƒé‡
    if mode == "replace":
        FEATURE_GROUPS.clear()
        DEFAULT_WEIGHTS.clear()
        # ä¹Ÿå…è®¸å›é€€åˆ°æç®€ï¼šè‹¥å¤–ç½®ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨å†…ç½®
        if not groups and not weights_override:
            FEATURE_GROUPS.update(deepcopy(BUILTIN_GROUPS))
            DEFAULT_WEIGHTS.update(deepcopy(BUILTIN_DEFAULT_WEIGHTS))
            mode = "replace-empty-fallback"

    # åˆå¹¶åˆ†ç»„å…³é”®è¯ï¼ˆå»é‡ï¼‰
    for group_name, key_list in groups.items():
        if not isinstance(key_list, list):
            continue
        existing = set(FEATURE_GROUPS.get(group_name, []))
        for k in key_list:
            try:
                k_l = str(k).strip()
            except Exception:
                continue
            if not k_l:
                continue
            existing.add(k_l)
        FEATURE_GROUPS[group_name] = sorted(existing)

    # å°†åˆ†ç»„æ–°å¢çš„å…³é”®è¯èµ‹é»˜è®¤æƒé‡
    for group_name, key_list in FEATURE_GROUPS.items():
        for k in key_list:
            if k not in DEFAULT_WEIGHTS:
                if group_name == "construction":
                    DEFAULT_WEIGHTS[k] = -2.0
                else:
                    DEFAULT_WEIGHTS[k] = 1.5

    # è‡ªå®šä¹‰æƒé‡ï¼šä¹Ÿå°†è¿™äº›è¯åŠ å…¥åˆ°ç‰¹å¾ç»„ï¼Œé¿å…åªåœ¨æƒé‡é‡Œä½†æ— æ³•è¢«æå–
    if weights_override:
        custom = set(FEATURE_GROUPS.get("_custom", []))
        for k, w in weights_override.items():
            try:
                k_l = str(k).strip()
                w_f = float(w)
            except Exception:
                continue
            if not k_l:
                continue
            DEFAULT_WEIGHTS[k_l] = w_f
            custom.add(k_l)
        FEATURE_GROUPS["_custom"] = sorted(custom)

    SEED_LOAD_SUMMARY.update(
        {
            "mode": mode,
            "seeds_groups": sum(len(v) for v in groups.values()) if groups else 0,
            "seeds_weights": len(weights_override),
            "final_features": len(DEFAULT_WEIGHTS),
            "tokenizer": "jieba",
            "tokenizer_effective": "jieba",
        }
    )
    # åˆå§‹åŒ–åˆ†è¯èµ„æºï¼ˆå›ºå®šä½¿ç”¨ jiebaï¼‰
    _init_tokenizer_resources()
    # æ„å»ºå…³é”®è¯ AC è‡ªåŠ¨æœº
    _rebuild_automaton()


# åœ¨æ¨¡å—å¯¼å…¥æ—¶åˆå¹¶å¤–ç½®ç§å­
_merge_seeds_into_defaults()


def get_seed_load_summary() -> str:
    m = SEED_LOAD_SUMMARY.get("mode", "")
    g = SEED_LOAD_SUMMARY.get("seeds_groups", 0)
    w = SEED_LOAD_SUMMARY.get("seeds_weights", 0)
    f = SEED_LOAD_SUMMARY.get("final_features", 0)
    t = SEED_LOAD_SUMMARY.get("tokenizer", "jieba")
    te = SEED_LOAD_SUMMARY.get("tokenizer_effective", "jieba")
    return f"å…³é”®è¯åŠ è½½: æ¨¡å¼={m}ï¼Œå¤–ç½®ç»„è¯={g}ï¼Œå¤–ç½®æƒé‡={w}ï¼Œæœ€ç»ˆç‰¹å¾æ•°={f}ï¼Œåˆ†è¯å™¨={t}({te})"


def sigmoid(x: float) -> float:
    if x < -50:
        return 0.0
    if x > 50:
        return 1.0
    return 1.0 / (1.0 + exp(-x))


def model_path_from_base(base_output_path: str) -> Path:
    p = Path(base_output_path)
    return p.parent / f"{p.stem}_hint_model.json"


def load_hint_model(base_output_path: str) -> Dict:
    path = model_path_from_base(base_output_path)
    if not path.exists():
        return {
            "bias": DEFAULT_BIAS,
            "weights": DEFAULT_WEIGHTS.copy(),
            "token_stats": {},  # token -> {"pos": int, "neg": int}
        }
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # ä¿æŠ¤æ€§åˆå¹¶ï¼ˆæ–°å…³é”®è¯åŠ å…¥æ—¶ï¼‰
        weights = DEFAULT_WEIGHTS.copy()
        weights.update(data.get("weights", {}))
        return {
            "bias": data.get("bias", DEFAULT_BIAS),
            "weights": weights,
            "token_stats": data.get("token_stats", {}),
        }
    except Exception:
        return {
            "bias": DEFAULT_BIAS,
            "weights": DEFAULT_WEIGHTS.copy(),
            "token_stats": {},
        }


def save_hint_model(base_output_path: str, model: Dict):
    path = model_path_from_base(base_output_path)
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(model, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def normalize_text(row) -> str:
    parts: List[str] = []
    for field in ("title", "category", "publish_date", "date"):
        if field in row.index and row[field] is not None:
            try:
                parts.append(str(row[field]))
            except Exception:
                pass
    # é™åˆ¶æ­£æ–‡å‰ 4000 å­—ï¼Œé¿å…ç‰¹åˆ«é•¿æ–‡æœ¬å½±å“æ€§èƒ½
    try:
        parts.append(str(row["full_text"])[:4000])
    except Exception:
        pass
    return "\n".join(parts).lower()


def extract_features(row) -> Dict[str, int]:
    text = normalize_text(row)
    t = text.lower()
    feats: Dict[str, int] = {}
    A = _AC
    if A is None:
        _rebuild_automaton()
        A = _AC
    if A is None:
        return feats
    matched: Set[str] = set()
    for _, payload in A.iter(t):
        # payload æ˜¯ lower å½¢å¼
        k = _AC_KEYMAP.get(payload, payload)
        matched.add(k)
    for k in matched:
        feats[k] = 1
    return feats


# ---------------- è‡ªå­¦ä¹ å…³é”®è¯ï¼ˆè½»é‡ï¼‰ ----------------


def _tokenize_for_learning(text: str) -> List[str]:
    # å›ºå®šä½¿ç”¨ jieba åˆ†è¯ï¼Œè¾…ä»¥è‹±æ–‡ã€æ•°å­—+ä¸­æ–‡æ¨¡å¼è¡¥å……
    tokens: List[str] = []
    # jieba ä¸­æ–‡/æ··åˆåˆ†è¯
    for w in jieba.lcut(text, cut_all=False):
        w = w.strip().lower()
        if not w:
            continue
        if w.isdigit():
            continue
        if w in _STOPWORDS:
            continue
        if len(w) < 2 or len(w) > MAX_TOKEN_LENGTH:
            continue
        tokens.append(w)
    # è‹±æ–‡/æ‹¼éŸ³å•è¯è¡¥å……ï¼ˆâ‰¥3ï¼‰
    t = text.lower()
    for m in re.finditer(r"[a-z]{3,}", t):
        tok = m.group(0)
        if tok in _STOPWORDS:
            continue
        if len(tok) <= MAX_TOKEN_LENGTH:
            tokens.append(tok)
    # æ•°å­—+ä¸­æ–‡/å­—æ¯ç»„åˆè¡¥å……ï¼ˆå¦‚ 588è½®ã€g123æ¬¡ï¼‰
    for m in re.finditer(r"\b\d{2,}[a-z\u4e00-\u9fff]+", t):
        tok = m.group(0)
        if len(tok) <= MAX_TOKEN_LENGTH:
            tokens.append(tok)
    return tokens


def update_token_stats(
    model: Dict, row, label_non_construction: int
) -> Dict[str, Tuple[int, int]]:
    """æ ¹æ®å½“å‰æ¡ˆä¾‹ä¸æ ‡ç­¾æ›´æ–° token ç»Ÿè®¡ï¼Œè¿”å›æœ¬æ¬¡å¢é‡ç”¨äºæ’¤é”€ã€‚
    label_non_construction: 1=éå»ºç­‘ä¸š, 0=å»ºç­‘ä¸š
    è¿”å›: {token: (d_pos, d_neg)}
    """
    if "token_stats" not in model:
        model["token_stats"] = {}
    text = normalize_text(row)
    toks = _tokenize_for_learning(text)
    delta: Dict[str, Tuple[int, int]] = {}
    for tok in toks:
        stat = model["token_stats"].setdefault(tok, {"pos": 0, "neg": 0})
        if label_non_construction == 1:
            stat["pos"] += 1
            delta[tok] = (1, 0)
        else:
            stat["neg"] += 1
            delta[tok] = (0, 1)
    return delta


def rollback_token_stats(model: Dict, delta: Dict[str, Tuple[int, int]]):
    if not delta:
        return
    stats = model.get("token_stats", {})
    for tok, (dp, dn) in delta.items():
        stat = stats.get(tok)
        if not stat:
            continue
        stat["pos"] = max(0, stat.get("pos", 0) - dp)
        stat["neg"] = max(0, stat.get("neg", 0) - dn)


def _log_odds(pos: int, neg: int, alpha: float = ALPHA) -> float:
    from math import log

    return log((pos + alpha) / (neg + alpha))


def maybe_expand_features(model: Dict, max_add: int = 3) -> List[str]:
    """åŸºäº token ç»Ÿè®¡ï¼Œç­›é€‰é«˜åˆ¤åˆ«åŠ›çš„ token åŠ¨æ€åŠ å…¥ä¸ºç‰¹å¾ï¼Œè¿”å›æ–°å¢åˆ—è¡¨ã€‚
    è§„åˆ™ï¼šæ€»é¢‘æ¬¡â‰¥MIN_COUNT ä¸” |log_odds|â‰¥é˜ˆå€¼ï¼›åˆå§‹æƒé‡=clip(log_odds, -3, 3)
    """
    stats = model.get("token_stats", {})
    weights = model.get("weights", {})
    candidates: List[Tuple[str, float, int]] = []  # (token, log_odds, total)
    for tok, st in stats.items():
        total = st.get("pos", 0) + st.get("neg", 0)
        if total < MIN_COUNT:
            continue
        lo = _log_odds(st.get("pos", 0), st.get("neg", 0))
        if abs(lo) >= ABS_LOG_ODDS_THRESH and tok not in weights:
            candidates.append((tok, lo, total))
    # å…ˆæŒ‰æ€»é¢‘æ¬¡ï¼Œå†æŒ‰ç»å¯¹ log_odds æ’åºï¼Œå–å‰è‹¥å¹²
    candidates.sort(key=lambda x: (x[2], abs(x[1])), reverse=True)
    added: List[str] = []
    # ç¡®ä¿å­˜åœ¨å­¦ä¹ ç»„
    learned_set = set(FEATURE_GROUPS.get("_learned", []))

    for tok, lo, _ in candidates[:max_add]:
        # åˆå§‹æƒé‡æŒ‰ log_odds æ˜ å°„ï¼Œå¹¶ç»“åˆæ–¹å‘ï¼šæ­£å€¼=éå»ºç­‘ä¸šï¼›è´Ÿå€¼=å»ºç­‘ä¸š
        init_w = max(-3.0, min(3.0, lo))
        weights[tok] = init_w
        learned_set.add(tok)
        added.append(tok)
    model["weights"] = weights
    FEATURE_GROUPS["_learned"] = sorted(learned_set)
    # æ–°å¢å­¦ä¹ ç‰¹å¾åé‡å»º AC è‡ªåŠ¨æœº
    if added:
        _rebuild_automaton()
    return added


def remove_learned_features(model: Dict, tokens: List[str]):
    """æ’¤é”€æ—¶ç§»é™¤æœ¬æ¬¡æ–°åŠ å…¥çš„ç‰¹å¾ï¼ˆä»…é™æœ¬æ¬¡æ–°å¢ï¼‰ã€‚"""
    if not tokens:
        return
    weights = model.get("weights", {})
    for t in tokens:
        if t in weights:
            # ä»…å½“è¯¥è¯æ¥è‡ªå­¦ä¹ ç»„ä¸”ä¸æ˜¯å†…ç½®/å¤–ç½®ç§å­æ—¶ç§»é™¤
            # ç®€å•ç­–ç•¥ï¼šå¦‚æœä¸åœ¨ä»»ä½•åˆ†ç»„ï¼ˆé™¤äº†_learnedå’Œ_customï¼‰åˆ™ç•™å­˜ï¼›å¦åˆ™ä¹Ÿå¯ä»¥ä¿ç•™ã€‚
            # è¿™é‡ŒæŒ‰ä¿å®ˆé€»è¾‘ï¼šåªä»weightsåˆ é™¤ï¼Œä¸åŠ¨åˆ†ç»„ä»¥é¿å…å½±å“å…¶ä»–æµç¨‹ã€‚
            try:
                del weights[t]
            except KeyError:
                pass
    model["weights"] = weights
    # ä»_learnedåˆ†ç»„ç§»é™¤
    learned = set(FEATURE_GROUPS.get("_learned", []))
    for t in tokens:
        learned.discard(t)
    FEATURE_GROUPS["_learned"] = sorted(learned)
    # å­¦ä¹ ç‰¹å¾ç§»é™¤åé‡å»º AC è‡ªåŠ¨æœº
    if tokens:
        _rebuild_automaton()


def predict_non_construction_proba(
    model: Dict, features: Dict[str, int]
) -> Tuple[float, List[Tuple[str, float]]]:
    w = model["weights"]
    z = model.get("bias", 0.0)
    contributions: List[Tuple[str, float]] = []
    for name, x in features.items():
        if not x:
            continue
        c = w.get(name, 0.0) * x
        contributions.append((name, c))
        z += c
    p = sigmoid(z)
    # æ ¹æ®ç»å¯¹è´¡çŒ®æ’åºï¼Œå–å‰å‡ é¡¹è§£é‡Š
    contributions.sort(key=lambda t: abs(t[1]), reverse=True)
    return p, contributions[:5]


def update_model_online(
    model: Dict, features: Dict[str, int], label_non_construction: int
) -> Dict[str, float]:
    # è¿”å›æœ¬æ¬¡åº”ç”¨åˆ°æƒé‡çš„å¢é‡ï¼ˆç”¨äºæ’¤é”€ï¼‰
    p, _ = predict_non_construction_proba(model, features)
    error = label_non_construction - p  # y - p
    delta_bias = LEARNING_RATE * error
    model["bias"] = model.get("bias", 0.0) + delta_bias
    delta_w: Dict[str, float] = {}
    for name, x in features.items():
        if not x:
            continue
        dw = LEARNING_RATE * error * x
        model["weights"][name] = model["weights"].get(name, 0.0) + dw
        delta_w[name] = dw
    # è¿”å›å¢é‡ä»¥ä¾¿æ’¤é”€
    return {"bias": delta_bias, "weights": delta_w}


def rollback_update(model: Dict, delta: Dict[str, float]):
    model["bias"] = model.get("bias", 0.0) - delta.get("bias", 0.0)
    for name, dw in delta.get("weights", {}).items():
        model["weights"][name] = model["weights"].get(name, 0.0) - dw


def format_hint_line(prob: float, contributors: List[Tuple[str, float]]) -> str:
    pct = int(round(prob * 100))
    reason_keys = [f for f, _ in contributors if _ != 0]
    if reason_keys:
        reason = "ï¼Œä¾æ®: " + ", ".join(reason_keys[:3])
    else:
        reason = ""
    return f"ğŸ” æ™ºèƒ½æç¤º: éå»ºç­‘ä¸šæ¦‚ç‡çº¦ {pct}%{reason}"


# ==================== å¢å¼ºç‰ˆï¼šåœ¨çº¿ TF-IDF + æ”¹è¿› LR ====================


class OnlineTFIDF:
    """åœ¨çº¿ TF-IDF ç‰¹å¾æå–å™¨ï¼ˆå¢é‡æ›´æ–°æ–‡æ¡£é¢‘ç‡ï¼‰"""

    def __init__(self, max_features: int = 300):
        self.max_features = max_features
        self.doc_count = 0
        self.term_doc_freq: Dict[str, int] = defaultdict(int)  # è¯åœ¨å¤šå°‘æ–‡æ¡£ä¸­å‡ºç°è¿‡
        self.vocabulary: Dict[str, int] = {}  # è¯ -> ç‰¹å¾ç´¢å¼•

    def learn_one(self, tokens: List[str]):
        """å¢é‡å­¦ä¹ ä¸€ä¸ªæ–‡æ¡£çš„è¯æ±‡"""
        self.doc_count += 1
        unique_tokens = set(tokens)

        # æ›´æ–°æ–‡æ¡£é¢‘ç‡
        for tok in unique_tokens:
            self.term_doc_freq[tok] += 1
            # åŠ¨æ€æ‰©å±•è¯è¡¨ï¼ˆé™åˆ¶å¤§å°ï¼‰
            if tok not in self.vocabulary and len(self.vocabulary) < self.max_features:
                self.vocabulary[tok] = len(self.vocabulary)

    def transform_one(self, tokens: List[str]) -> Dict[str, float]:
        """å°†æ–‡æ¡£è½¬æ¢ä¸º TF-IDF ç‰¹å¾å‘é‡"""
        if self.doc_count == 0:
            return {}

        # è®¡ç®— TFï¼ˆè¯é¢‘ï¼‰
        term_count = defaultdict(int)
        for tok in tokens:
            if tok in self.vocabulary:
                term_count[tok] += 1

        total_terms = len(tokens)
        if total_terms == 0:
            return {}

        # è®¡ç®— TF-IDF
        tfidf_feats: Dict[str, float] = {}
        for tok, count in term_count.items():
            if tok not in self.vocabulary:
                continue

            tf = count / total_terms
            df = self.term_doc_freq.get(tok, 1)
            # IDF = log((N + 1) / (df + 1))
            idf = log((self.doc_count + 1) / (df + 1))
            tfidf_feats[f"tfidf_{tok}"] = tf * idf

        # L2 å½’ä¸€åŒ–
        norm = sqrt(sum(v * v for v in tfidf_feats.values()))
        if norm > 0:
            tfidf_feats = {k: v / norm for k, v in tfidf_feats.items()}

        return tfidf_feats

    def to_dict(self) -> Dict:
        return {
            "doc_count": self.doc_count,
            "term_doc_freq": dict(self.term_doc_freq),
            "vocabulary": self.vocabulary,
            "max_features": self.max_features,
        }

    @classmethod
    def from_dict(cls, data: Dict):
        obj = cls(max_features=data.get("max_features", 300))
        obj.doc_count = data.get("doc_count", 0)
        obj.term_doc_freq = defaultdict(int, data.get("term_doc_freq", {}))
        obj.vocabulary = data.get("vocabulary", {})
        return obj


def extract_features_enhanced(model: Dict, row) -> Dict[str, float]:
    """å¢å¼ºç‰ˆç‰¹å¾æå–ï¼šå…³é”®è¯ç‰¹å¾ + TF-IDF ç‰¹å¾"""
    # 1. åŸæœ‰å…³é”®è¯ç‰¹å¾ï¼ˆäºŒå€¼ï¼‰
    keyword_feats = extract_features(row)

    # 2. TF-IDF ç‰¹å¾ï¼ˆè¿ç»­å€¼ï¼‰
    tfidf_module = model.get("tfidf")
    if tfidf_module is None:
        # é¦–æ¬¡ä½¿ç”¨ï¼Œåˆå§‹åŒ–
        tfidf_module = OnlineTFIDF(max_features=300)
        model["tfidf"] = tfidf_module

    text = normalize_text(row)
    tokens = _tokenize_for_learning(text)
    tfidf_feats = tfidf_module.transform_one(tokens)

    # åˆå¹¶ç‰¹å¾ï¼ˆå…³é”®è¯æƒé‡ä¸º1ï¼ŒTF-IDFæƒé‡ä¸ºå®é™…å€¼ï¼‰
    all_feats: Dict[str, float] = {}
    for k, v in keyword_feats.items():
        all_feats[k] = float(v)
    all_feats.update(tfidf_feats)

    return all_feats


def predict_non_construction_proba_enhanced(
    model: Dict, features: Dict[str, float]
) -> Tuple[float, List[Tuple[str, float]]]:
    """å¢å¼ºç‰ˆé¢„æµ‹ï¼ˆæ”¯æŒè¿ç»­å€¼ç‰¹å¾ï¼‰"""
    w = model["weights"]
    z = model.get("bias", 0.0)
    contributions: List[Tuple[str, float]] = []

    for name, x in features.items():
        if x == 0:
            continue
        weight = w.get(name, 0.0)
        c = weight * x
        contributions.append((name, c))
        z += c

    p = sigmoid(z)
    contributions.sort(key=lambda t: abs(t[1]), reverse=True)
    return p, contributions[:5]


def update_model_online_enhanced(
    model: Dict, row, features: Dict[str, float], label_non_construction: int
) -> Dict:
    """å¢å¼ºç‰ˆåœ¨çº¿æ›´æ–°ï¼ˆL2æ­£åˆ™åŒ– + è‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰"""
    # è®°å½•è®­ç»ƒæ¬¡æ•°ï¼ˆç”¨äºè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
    n_updates = model.get("n_updates", 0) + 1
    model["n_updates"] = n_updates

    # è‡ªé€‚åº”å­¦ä¹ ç‡ï¼šlr_t = lr_0 / sqrt(t)
    adaptive_lr = LEARNING_RATE / sqrt(n_updates)

    # L2 æ­£åˆ™åŒ–ç³»æ•°
    l2_lambda = 0.01

    # é¢„æµ‹
    p, _ = predict_non_construction_proba_enhanced(model, features)
    error = label_non_construction - p

    # æ›´æ–°åç½®
    delta_bias = adaptive_lr * error
    model["bias"] = model.get("bias", 0.0) + delta_bias

    # æ›´æ–°æƒé‡ï¼ˆå¸¦ L2 æ­£åˆ™åŒ–ï¼‰
    delta_w: Dict[str, float] = {}
    for name, x in features.items():
        if x == 0:
            continue
        old_w = model["weights"].get(name, 0.0)
        # æ¢¯åº¦ = error * x - l2_lambda * w
        gradient = error * x - l2_lambda * old_w
        dw = adaptive_lr * gradient
        model["weights"][name] = old_w + dw
        delta_w[name] = dw

    # æ›´æ–° TF-IDF æ¨¡å—
    tfidf_module = model.get("tfidf")
    if tfidf_module is not None:
        text = normalize_text(row)
        tokens = _tokenize_for_learning(text)
        tfidf_module.learn_one(tokens)

    return {"bias": delta_bias, "weights": delta_w}


def load_hint_model_enhanced(base_output_path: str) -> Dict:
    """åŠ è½½å¢å¼ºç‰ˆæ¨¡å‹ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰"""
    model = load_hint_model(base_output_path)

    # å¦‚æœå­˜åœ¨ TF-IDF æ•°æ®ï¼Œæ¢å¤
    if "tfidf" in model and isinstance(model["tfidf"], dict):
        model["tfidf"] = OnlineTFIDF.from_dict(model["tfidf"])
    else:
        model["tfidf"] = OnlineTFIDF(max_features=300)

    if "n_updates" not in model:
        model["n_updates"] = 0

    return model


def save_hint_model_enhanced(base_output_path: str, model: Dict):
    """ä¿å­˜å¢å¼ºç‰ˆæ¨¡å‹"""
    # åºåˆ—åŒ– TF-IDF æ¨¡å—
    model_copy = model.copy()
    tfidf_module = model_copy.get("tfidf")
    if tfidf_module is not None and hasattr(tfidf_module, "to_dict"):
        model_copy["tfidf"] = tfidf_module.to_dict()

    save_hint_model(base_output_path, model_copy)
